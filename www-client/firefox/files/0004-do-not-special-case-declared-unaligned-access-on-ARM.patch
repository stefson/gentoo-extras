# HG changeset patch
# User Lars T Hansen <lhansen@mozilla.com>
# Date 1628235677 0
# Node ID 48adeca98451cdb2e6810faf0ab0c3e741ff2a24
# Parent  ff4b90400ed336df7fde8a6180880abb31bad752
Bug 1723930 - Do not special-case declared-unaligned accesses on ARM. r=jseward

Since early on, code generation on ARM has emulated accesses that are
declared as unaligned (the align= attribute has a value that is less
than the natural alignment for the datum) by emitting individual byte
accesses.  There's almost no good reason to continue to do this, as we
are completely dependent on other mechanisms for handling accesses
that are actually unaligned (the effective address does not have
natural alignment) though declared to be aligned:

- For integer accesses we are dependent on the hardware/OS to support
  actually-unaligned accesses transparently.

- For FP accesses we handle signals originating from
  actually-unaligned accesses that are not handled by hardware/OS, but
  that code is being rewritten over in bug 1587757 to avoid the trap
  handling altogether by using unaligned-friendly instructions.

(Some of the code ripped out in this patch comes back in the patch for
bug 1587757, though in a different form.)

The only reason to continue using individual byte accesses would be
performance: if the CPU does not support unaligned accesses and the OS
has to emulate them in a trap handler, programs could be faster by
avoiding the traps.  Yet such hardware is increasingly nonexistent.
Also, this solution would depend on the tools to declare accesses as
potentially-unaligned in all appropriate cases, and that is almost
certainly not going to happen.

Differential Revision: https://phabricator.services.mozilla.com/D121832

diff --git a/js/src/jit/MacroAssembler.h b/js/src/jit/MacroAssembler.h
--- a/js/src/jit/MacroAssembler.h
+++ b/js/src/jit/MacroAssembler.h
@@ -3578,52 +3578,48 @@ class MacroAssembler : public MacroAssem
                  Register memoryBase, Register ptr) DEFINED_ON(arm64);
   void wasmStoreI64(const wasm::MemoryAccessDesc& access, Register64 value,
                     Register memoryBase, Register ptr) DEFINED_ON(arm64);
 
   // `ptr` will always be updated.
   void wasmUnalignedLoad(const wasm::MemoryAccessDesc& access,
                          Register memoryBase, Register ptr, Register ptrScratch,
                          Register output, Register tmp)
-      DEFINED_ON(arm, mips32, mips64);
-
-  // ARM: `ptr` will always be updated and `tmp1` is always needed.  `tmp2` is
-  // needed for Float32; `tmp2` and `tmp3` are needed for Float64.  Temps must
-  // be Invalid when they are not needed.
+      DEFINED_ON(mips32, mips64);
+
   // MIPS: `ptr` will always be updated.
   void wasmUnalignedLoadFP(const wasm::MemoryAccessDesc& access,
                            Register memoryBase, Register ptr,
                            Register ptrScratch, FloatRegister output,
                            Register tmp1, Register tmp2, Register tmp3)
-      DEFINED_ON(arm, mips32, mips64);
+      DEFINED_ON(mips32, mips64);
 
   // `ptr` will always be updated.
   void wasmUnalignedLoadI64(const wasm::MemoryAccessDesc& access,
                             Register memoryBase, Register ptr,
                             Register ptrScratch, Register64 output,
-                            Register tmp) DEFINED_ON(arm, mips32, mips64);
-
-  // ARM: `ptr` and `value` will always be updated.  'tmp' must be Invalid.
+                            Register tmp) DEFINED_ON(mips32, mips64);
+
   // MIPS: `ptr` will always be updated.
   void wasmUnalignedStore(const wasm::MemoryAccessDesc& access, Register value,
                           Register memoryBase, Register ptr,
                           Register ptrScratch, Register tmp)
-      DEFINED_ON(arm, mips32, mips64);
+      DEFINED_ON(mips32, mips64);
 
   // `ptr` will always be updated.
   void wasmUnalignedStoreFP(const wasm::MemoryAccessDesc& access,
                             FloatRegister floatValue, Register memoryBase,
                             Register ptr, Register ptrScratch, Register tmp)
-      DEFINED_ON(arm, mips32, mips64);
+      DEFINED_ON(mips32, mips64);
 
   // `ptr` will always be updated.
   void wasmUnalignedStoreI64(const wasm::MemoryAccessDesc& access,
                              Register64 value, Register memoryBase,
                              Register ptr, Register ptrScratch, Register tmp)
-      DEFINED_ON(arm, mips32, mips64);
+      DEFINED_ON(mips32, mips64);
 
   // wasm specific methods, used in both the wasm baseline compiler and ion.
 
   // The truncate-to-int32 methods do not bind the rejoin label; clients must
   // do so if oolWasmTruncateCheckF64ToI32() can jump to it.
   void wasmTruncateDoubleToUInt32(FloatRegister input, Register output,
                                   bool isSaturating, Label* oolEntry) PER_ARCH;
   void wasmTruncateDoubleToInt32(FloatRegister input, Register output,
diff --git a/js/src/jit/arm/CodeGenerator-arm.cpp b/js/src/jit/arm/CodeGenerator-arm.cpp
--- a/js/src/jit/arm/CodeGenerator-arm.cpp
+++ b/js/src/jit/arm/CodeGenerator-arm.cpp
@@ -2025,49 +2025,16 @@ void CodeGeneratorARM::emitWasmLoad(T* l
                   ToAnyRegister(lir->output()));
   }
 }
 
 void CodeGenerator::visitWasmLoad(LWasmLoad* lir) { emitWasmLoad(lir); }
 
 void CodeGenerator::visitWasmLoadI64(LWasmLoadI64* lir) { emitWasmLoad(lir); }
 
-template <typename T>
-void CodeGeneratorARM::emitWasmUnalignedLoad(T* lir) {
-  const MWasmLoad* mir = lir->mir();
-  MIRType resultType = mir->type();
-
-  Register ptr = ToRegister(lir->ptrCopy());
-  Register tmp1 = ToRegister(lir->getTemp(1));
-
-  if (resultType == MIRType::Int64) {
-    masm.wasmUnalignedLoadI64(mir->access(), HeapReg, ptr, ptr,
-                              ToOutRegister64(lir), tmp1);
-  } else if (IsFloatingPointType(resultType)) {
-    Register tmp2(ToRegister(lir->getTemp(2)));
-    Register tmp3(Register::Invalid());
-    if (mir->access().byteSize() == 8) {
-      tmp3 = ToRegister(lir->getTemp(3));
-    }
-    masm.wasmUnalignedLoadFP(mir->access(), HeapReg, ptr, ptr,
-                             ToFloatRegister(lir->output()), tmp1, tmp2, tmp3);
-  } else {
-    masm.wasmUnalignedLoad(mir->access(), HeapReg, ptr, ptr,
-                           ToRegister(lir->output()), tmp1);
-  }
-}
-
-void CodeGenerator::visitWasmUnalignedLoad(LWasmUnalignedLoad* lir) {
-  emitWasmUnalignedLoad(lir);
-}
-
-void CodeGenerator::visitWasmUnalignedLoadI64(LWasmUnalignedLoadI64* lir) {
-  emitWasmUnalignedLoad(lir);
-}
-
 void CodeGenerator::visitWasmAddOffset(LWasmAddOffset* lir) {
   MWasmAddOffset* mir = lir->mir();
   Register base = ToRegister(lir->base());
   Register out = ToRegister(lir->output());
 
   ScratchRegisterScope scratch(masm);
   masm.ma_add(base, Imm32(mir->offset()), out, scratch, SetCC);
 
@@ -2103,47 +2070,16 @@ void CodeGeneratorARM::emitWasmStore(T* 
 }
 
 void CodeGenerator::visitWasmStore(LWasmStore* lir) { emitWasmStore(lir); }
 
 void CodeGenerator::visitWasmStoreI64(LWasmStoreI64* lir) {
   emitWasmStore(lir);
 }
 
-template <typename T>
-void CodeGeneratorARM::emitWasmUnalignedStore(T* lir) {
-  const MWasmStore* mir = lir->mir();
-  MIRType valueType = mir->value()->type();
-  Register ptr = ToRegister(lir->ptrCopy());
-  Register valOrTmp = ToRegister(lir->valueHelper());
-
-  if (valueType == MIRType::Int64) {
-    masm.wasmUnalignedStoreI64(
-        mir->access(),
-        ToRegister64(lir->getInt64Operand(LWasmUnalignedStoreI64::ValueIndex)),
-        HeapReg, ptr, ptr, valOrTmp);
-  } else if (valueType == MIRType::Float32 || valueType == MIRType::Double) {
-    FloatRegister value =
-        ToFloatRegister(lir->getOperand(LWasmUnalignedStore::ValueIndex));
-    masm.wasmUnalignedStoreFP(mir->access(), value, HeapReg, ptr, ptr,
-                              valOrTmp);
-  } else {
-    masm.wasmUnalignedStore(mir->access(), valOrTmp, HeapReg, ptr, ptr,
-                            Register::Invalid());
-  }
-}
-
-void CodeGenerator::visitWasmUnalignedStore(LWasmUnalignedStore* lir) {
-  emitWasmUnalignedStore(lir);
-}
-
-void CodeGenerator::visitWasmUnalignedStoreI64(LWasmUnalignedStoreI64* lir) {
-  emitWasmUnalignedStore(lir);
-}
-
 void CodeGenerator::visitAsmJSStoreHeap(LAsmJSStoreHeap* ins) {
   const MAsmJSStoreHeap* mir = ins->mir();
 
   const LAllocation* ptr = ins->ptr();
   const LAllocation* boundsCheckLimit = ins->boundsCheckLimit();
 
   bool isSigned;
   int size;
diff --git a/js/src/jit/arm/LIR-arm.h b/js/src/jit/arm/LIR-arm.h
--- a/js/src/jit/arm/LIR-arm.h
+++ b/js/src/jit/arm/LIR-arm.h
@@ -397,113 +397,16 @@ class LInt64ToFloatingPointCall
   LAllocation* input() { return getOperand(Input); }
   LAllocation* tls() { return getOperand(Tls); }
 
   MBuiltinInt64ToFloatingPoint* mir() const {
     return mir_->toBuiltinInt64ToFloatingPoint();
   }
 };
 
-namespace details {
-
-// Base class for the int64 and non-int64 variants.
-template <size_t NumDefs>
-class LWasmUnalignedLoadBase : public details::LWasmLoadBase<NumDefs, 4> {
- public:
-  typedef LWasmLoadBase<NumDefs, 4> Base;
-  explicit LWasmUnalignedLoadBase(LNode::Opcode opcode, const LAllocation& ptr,
-                                  const LDefinition& ptrCopy,
-                                  const LDefinition& temp1,
-                                  const LDefinition& temp2,
-                                  const LDefinition& temp3)
-      : Base(opcode, ptr, LAllocation()) {
-    Base::setTemp(0, ptrCopy);
-    Base::setTemp(1, temp1);
-    Base::setTemp(2, temp2);
-    Base::setTemp(3, temp3);
-  }
-
-  const LDefinition* ptrCopy() { return Base::getTemp(0); }
-};
-
-}  // namespace details
-
-class LWasmUnalignedLoad : public details::LWasmUnalignedLoadBase<1> {
- public:
-  explicit LWasmUnalignedLoad(const LAllocation& ptr,
-                              const LDefinition& ptrCopy,
-                              const LDefinition& temp1,
-                              const LDefinition& temp2,
-                              const LDefinition& temp3)
-      : LWasmUnalignedLoadBase(classOpcode, ptr, ptrCopy, temp1, temp2, temp3) {
-  }
-  LIR_HEADER(WasmUnalignedLoad);
-};
-
-class LWasmUnalignedLoadI64
-    : public details::LWasmUnalignedLoadBase<INT64_PIECES> {
- public:
-  explicit LWasmUnalignedLoadI64(const LAllocation& ptr,
-                                 const LDefinition& ptrCopy,
-                                 const LDefinition& temp1,
-                                 const LDefinition& temp2,
-                                 const LDefinition& temp3)
-      : LWasmUnalignedLoadBase(classOpcode, ptr, ptrCopy, temp1, temp2, temp3) {
-  }
-  LIR_HEADER(WasmUnalignedLoadI64);
-};
-
-namespace details {
-
-// Base class for the int64 and non-int64 variants.
-template <size_t NumOps>
-class LWasmUnalignedStoreBase : public LInstructionHelper<0, NumOps, 2> {
- public:
-  typedef LInstructionHelper<0, NumOps, 2> Base;
-
-  static const uint32_t ValueIndex = 1;
-
-  LWasmUnalignedStoreBase(LNode::Opcode opcode, const LAllocation& ptr,
-                          const LDefinition& ptrCopy,
-                          const LDefinition& valueHelper)
-      : Base(opcode) {
-    Base::setOperand(0, ptr);
-    Base::setTemp(0, ptrCopy);
-    Base::setTemp(1, valueHelper);
-  }
-  MWasmStore* mir() const { return Base::mir_->toWasmStore(); }
-  const LDefinition* ptrCopy() { return Base::getTemp(0); }
-  const LDefinition* valueHelper() { return Base::getTemp(1); }
-};
-
-}  // namespace details
-
-class LWasmUnalignedStore : public details::LWasmUnalignedStoreBase<2> {
- public:
-  LIR_HEADER(WasmUnalignedStore);
-  LWasmUnalignedStore(const LAllocation& ptr, const LAllocation& value,
-                      const LDefinition& ptrCopy,
-                      const LDefinition& valueHelper)
-      : LWasmUnalignedStoreBase(classOpcode, ptr, ptrCopy, valueHelper) {
-    setOperand(1, value);
-  }
-};
-
-class LWasmUnalignedStoreI64
-    : public details::LWasmUnalignedStoreBase<1 + INT64_PIECES> {
- public:
-  LIR_HEADER(WasmUnalignedStoreI64);
-  LWasmUnalignedStoreI64(const LAllocation& ptr, const LInt64Allocation& value,
-                         const LDefinition& ptrCopy,
-                         const LDefinition& valueHelper)
-      : LWasmUnalignedStoreBase(classOpcode, ptr, ptrCopy, valueHelper) {
-    setInt64Operand(1, value);
-  }
-};
-
 class LWasmAtomicLoadI64 : public LInstructionHelper<INT64_PIECES, 1, 0> {
  public:
   LIR_HEADER(WasmAtomicLoadI64);
 
   explicit LWasmAtomicLoadI64(const LAllocation& ptr)
       : LInstructionHelper(classOpcode) {
     setOperand(0, ptr);
   }
diff --git a/js/src/jit/arm/Lowering-arm.cpp b/js/src/jit/arm/Lowering-arm.cpp
--- a/js/src/jit/arm/Lowering-arm.cpp
+++ b/js/src/jit/arm/Lowering-arm.cpp
@@ -677,47 +677,16 @@ void LIRGenerator::visitWasmLoad(MWasmLo
     defineInt64Fixed(lir, ins,
                      LInt64Allocation(LAllocation(AnyRegister(IntArgReg1)),
                                       LAllocation(AnyRegister(IntArgReg0))));
     return;
   }
 
   LAllocation ptr = useRegisterAtStart(base);
 
-  if (IsUnaligned(ins->access())) {
-    MOZ_ASSERT(!ins->access().isAtomic());
-
-    // Unaligned access expected! Revert to a byte load.
-    LDefinition ptrCopy = tempCopy(base, 0);
-
-    LDefinition noTemp = LDefinition::BogusTemp();
-    if (ins->type() == MIRType::Int64) {
-      auto* lir = new (alloc())
-          LWasmUnalignedLoadI64(ptr, ptrCopy, temp(), noTemp, noTemp);
-      defineInt64(lir, ins);
-      return;
-    }
-
-    LDefinition temp2 = noTemp;
-    LDefinition temp3 = noTemp;
-    if (IsFloatingPointType(ins->type())) {
-      // For putting the low value in a GPR.
-      temp2 = temp();
-      // For putting the high value in a GPR.
-      if (ins->type() == MIRType::Double) {
-        temp3 = temp();
-      }
-    }
-
-    auto* lir =
-        new (alloc()) LWasmUnalignedLoad(ptr, ptrCopy, temp(), temp2, temp3);
-    define(lir, ins);
-    return;
-  }
-
   if (ins->type() == MIRType::Int64) {
     auto* lir = new (alloc()) LWasmLoadI64(ptr);
     if (ins->access().offset() || ins->access().type() == Scalar::Int64) {
       lir->setTemp(0, tempCopy(base, 0));
     }
     defineInt64(lir, ins);
     return;
   }
@@ -740,42 +709,16 @@ void LIRGenerator::visitWasmStore(MWasmS
         useInt64Fixed(ins->value(), Register64(IntArgReg1, IntArgReg0)),
         tempFixed(IntArgReg2), tempFixed(IntArgReg3));
     add(lir, ins);
     return;
   }
 
   LAllocation ptr = useRegisterAtStart(base);
 
-  if (IsUnaligned(ins->access())) {
-    MOZ_ASSERT(!ins->access().isAtomic());
-
-    // Unaligned access expected! Revert to a byte store.
-    LDefinition ptrCopy = tempCopy(base, 0);
-
-    MIRType valueType = ins->value()->type();
-    if (valueType == MIRType::Int64) {
-      LInt64Allocation value = useInt64RegisterAtStart(ins->value());
-      auto* lir =
-          new (alloc()) LWasmUnalignedStoreI64(ptr, value, ptrCopy, temp());
-      add(lir, ins);
-      return;
-    }
-
-    LAllocation value = useRegisterAtStart(ins->value());
-    LDefinition valueHelper = IsFloatingPointType(valueType)
-                                  ? temp()  // to do a FPU -> GPR move.
-                                  : tempCopy(base, 1);  // to clobber the value.
-
-    auto* lir =
-        new (alloc()) LWasmUnalignedStore(ptr, value, ptrCopy, valueHelper);
-    add(lir, ins);
-    return;
-  }
-
   if (ins->value()->type() == MIRType::Int64) {
     LInt64Allocation value = useInt64RegisterAtStart(ins->value());
     auto* lir = new (alloc()) LWasmStoreI64(ptr, value);
     if (ins->access().offset() || ins->access().type() == Scalar::Int64) {
       lir->setTemp(0, tempCopy(base, 0));
     }
     add(lir, ins);
     return;
diff --git a/js/src/jit/arm/MacroAssembler-arm.cpp b/js/src/jit/arm/MacroAssembler-arm.cpp
--- a/js/src/jit/arm/MacroAssembler-arm.cpp
+++ b/js/src/jit/arm/MacroAssembler-arm.cpp
@@ -4835,67 +4835,16 @@ void MacroAssembler::wasmStore(const was
 
 void MacroAssembler::wasmStoreI64(const wasm::MemoryAccessDesc& access,
                                   Register64 value, Register memoryBase,
                                   Register ptr, Register ptrScratch) {
   MOZ_ASSERT(!access.isAtomic());
   wasmStoreImpl(access, AnyRegister(), value, memoryBase, ptr, ptrScratch);
 }
 
-void MacroAssembler::wasmUnalignedLoad(const wasm::MemoryAccessDesc& access,
-                                       Register memoryBase, Register ptr,
-                                       Register ptrScratch, Register output,
-                                       Register tmp) {
-  wasmUnalignedLoadImpl(access, memoryBase, ptr, ptrScratch,
-                        AnyRegister(output), Register64::Invalid(), tmp,
-                        Register::Invalid(), Register::Invalid());
-}
-
-void MacroAssembler::wasmUnalignedLoadFP(const wasm::MemoryAccessDesc& access,
-                                         Register memoryBase, Register ptr,
-                                         Register ptrScratch,
-                                         FloatRegister outFP, Register tmp1,
-                                         Register tmp2, Register tmp3) {
-  wasmUnalignedLoadImpl(access, memoryBase, ptr, ptrScratch, AnyRegister(outFP),
-                        Register64::Invalid(), tmp1, tmp2, tmp3);
-}
-
-void MacroAssembler::wasmUnalignedLoadI64(const wasm::MemoryAccessDesc& access,
-                                          Register memoryBase, Register ptr,
-                                          Register ptrScratch, Register64 out64,
-                                          Register tmp) {
-  wasmUnalignedLoadImpl(access, memoryBase, ptr, ptrScratch, AnyRegister(),
-                        out64, tmp, Register::Invalid(), Register::Invalid());
-}
-
-void MacroAssembler::wasmUnalignedStore(const wasm::MemoryAccessDesc& access,
-                                        Register value, Register memoryBase,
-                                        Register ptr, Register ptrScratch,
-                                        Register tmp) {
-  MOZ_ASSERT(tmp == Register::Invalid());
-  wasmUnalignedStoreImpl(access, FloatRegister(), Register64::Invalid(),
-                         memoryBase, ptr, ptrScratch, value);
-}
-
-void MacroAssembler::wasmUnalignedStoreFP(const wasm::MemoryAccessDesc& access,
-                                          FloatRegister floatVal,
-                                          Register memoryBase, Register ptr,
-                                          Register ptrScratch, Register tmp) {
-  wasmUnalignedStoreImpl(access, floatVal, Register64::Invalid(), memoryBase,
-                         ptr, ptrScratch, tmp);
-}
-
-void MacroAssembler::wasmUnalignedStoreI64(const wasm::MemoryAccessDesc& access,
-                                           Register64 val64,
-                                           Register memoryBase, Register ptr,
-                                           Register ptrScratch, Register tmp) {
-  wasmUnalignedStoreImpl(access, FloatRegister(), val64, memoryBase, ptr,
-                         ptrScratch, tmp);
-}
-
 // ========================================================================
 // Primitive atomic operations.
 
 static Register ComputePointerForAtomic(MacroAssembler& masm,
                                         const BaseIndex& src, Register r) {
   Register base = src.base;
   Register index = src.index;
   uint32_t scale = Imm32::ShiftOf(src.scale).value;
@@ -6288,247 +6237,8 @@ void MacroAssemblerARM::wasmStoreImpl(co
       store = ma_dataTransferN(IsStore, 8 * byteSize /* bits */, isSigned,
                                memoryBase, ptr, val);
       append(access, store.getOffset());
     }
   }
 
   asMasm().memoryBarrierAfter(access.sync());
 }
-
-void MacroAssemblerARM::wasmUnalignedLoadImpl(
-    const wasm::MemoryAccessDesc& access, Register memoryBase, Register ptr,
-    Register ptrScratch, AnyRegister outAny, Register64 out64, Register tmp,
-    Register tmp2, Register tmp3) {
-  MOZ_ASSERT(ptr == ptrScratch);
-  MOZ_ASSERT(tmp != ptr);
-  MOZ_ASSERT(!Assembler::SupportsFastUnalignedAccesses());
-  MOZ_ASSERT(!access.isZeroExtendSimd128Load());
-  MOZ_ASSERT(!access.isSplatSimd128Load());
-  MOZ_ASSERT(!access.isWidenSimd128Load());
-
-  uint32_t offset = access.offset();
-  MOZ_ASSERT(offset < asMasm().wasmMaxOffsetGuardLimit());
-
-  if (offset) {
-    ScratchRegisterScope scratch(asMasm());
-    ma_add(Imm32(offset), ptr, scratch);
-  }
-
-  // Add memoryBase to ptr, so we can use base+index addressing in the byte
-  // loads.
-  ma_add(memoryBase, ptr);
-
-  unsigned byteSize = access.byteSize();
-  MOZ_ASSERT(byteSize == 8 || byteSize == 4 || byteSize == 2);
-
-  Scalar::Type type = access.type();
-  bool isSigned =
-      type == Scalar::Int16 || type == Scalar::Int32 || type == Scalar::Int64;
-
-  // If it's a two-word load we must load the high word first to get signal
-  // handling right.
-
-  asMasm().memoryBarrierBefore(access.sync());
-
-  switch (access.type()) {
-    case Scalar::Float32: {
-      MOZ_ASSERT(byteSize == 4);
-      MOZ_ASSERT(tmp2 != Register::Invalid() && tmp3 == Register::Invalid());
-      MOZ_ASSERT(outAny.fpu().isSingle());
-      emitUnalignedLoad(&access, /*signed*/ false, /*size*/ 4, ptr, tmp, tmp2);
-      ma_vxfer(tmp2, outAny.fpu());
-      break;
-    }
-    case Scalar::Float64: {
-      MOZ_ASSERT(byteSize == 8);
-      MOZ_ASSERT(tmp2 != Register::Invalid() && tmp3 != Register::Invalid());
-      MOZ_ASSERT(outAny.fpu().isDouble());
-      emitUnalignedLoad(&access, /*signed=*/false, /*size=*/4, ptr, tmp, tmp3,
-                        /*offset=*/4);
-      emitUnalignedLoad(nullptr, /*signed=*/false, /*size=*/4, ptr, tmp, tmp2);
-      ma_vxfer(tmp2, tmp3, outAny.fpu());
-      break;
-    }
-    case Scalar::Int64: {
-      MOZ_ASSERT(byteSize == 8);
-      MOZ_ASSERT(tmp2 == Register::Invalid() && tmp3 == Register::Invalid());
-      MOZ_ASSERT(out64.high != ptr);
-      emitUnalignedLoad(&access, /*signed=*/false, /*size=*/4, ptr, tmp,
-                        out64.high, /*offset=*/4);
-      emitUnalignedLoad(nullptr, /*signed=*/false, /*size=*/4, ptr, tmp,
-                        out64.low);
-      break;
-    }
-    case Scalar::Int16:
-    case Scalar::Uint16:
-    case Scalar::Int32:
-    case Scalar::Uint32: {
-      MOZ_ASSERT(byteSize <= 4);
-      MOZ_ASSERT(tmp2 == Register::Invalid() && tmp3 == Register::Invalid());
-      if (out64 != Register64::Invalid()) {
-        emitUnalignedLoad(&access, isSigned, byteSize, ptr, tmp, out64.low);
-        if (isSigned) {
-          ma_asr(Imm32(31), out64.low, out64.high);
-        } else {
-          ma_mov(Imm32(0), out64.high);
-        }
-      } else {
-        emitUnalignedLoad(&access, isSigned, byteSize, ptr, tmp, outAny.gpr());
-      }
-      break;
-    }
-    case Scalar::Int8:
-    case Scalar::Uint8:
-    default: {
-      MOZ_CRASH("Bad type");
-    }
-  }
-
-  asMasm().memoryBarrierAfter(access.sync());
-}
-
-void MacroAssemblerARM::wasmUnalignedStoreImpl(
-    const wasm::MemoryAccessDesc& access, FloatRegister floatValue,
-    Register64 val64, Register memoryBase, Register ptr, Register ptrScratch,
-    Register valOrTmp) {
-  MOZ_ASSERT(ptr == ptrScratch);
-  // They can't both be valid, but they can both be invalid.
-  MOZ_ASSERT(floatValue.isInvalid() || val64 == Register64::Invalid());
-  // Don't try extremely clever optimizations.
-  MOZ_ASSERT_IF(val64 != Register64::Invalid(),
-                valOrTmp != val64.high && valOrTmp != val64.low);
-
-  uint32_t offset = access.offset();
-  MOZ_ASSERT(offset < asMasm().wasmMaxOffsetGuardLimit());
-
-  unsigned byteSize = access.byteSize();
-  MOZ_ASSERT(byteSize == 8 || byteSize == 4 || byteSize == 2);
-
-  if (offset) {
-    ScratchRegisterScope scratch(asMasm());
-    ma_add(Imm32(offset), ptr, scratch);
-  }
-
-  // Add memoryBase to ptr, so we can use base+index addressing in the byte
-  // loads.
-  ma_add(memoryBase, ptr);
-
-  asMasm().memoryBarrierAfter(access.sync());
-
-  // If it's a two-word store we must store the high word first to get signal
-  // handling right.
-
-  if (val64 != Register64::Invalid()) {
-    if (byteSize == 8) {
-      emitUnalignedStore(&access, /*size=*/4, ptr, val64.high, /*offset=*/4);
-      emitUnalignedStore(nullptr, /*size=*/4, ptr, val64.low);
-    } else {
-      emitUnalignedStore(&access, byteSize, ptr, val64.low);
-    }
-  } else if (!floatValue.isInvalid()) {
-    if (floatValue.isDouble()) {
-      MOZ_ASSERT(byteSize == 8);
-      ScratchRegisterScope scratch(asMasm());
-      ma_vxfer(floatValue, scratch, valOrTmp);
-      emitUnalignedStore(&access, /*size=*/4, ptr, valOrTmp, /*offset=*/4);
-      emitUnalignedStore(nullptr, /*size=*/4, ptr, scratch);
-    } else {
-      MOZ_ASSERT(byteSize == 4);
-      ma_vxfer(floatValue, valOrTmp);
-      emitUnalignedStore(&access, /*size=*/4, ptr, valOrTmp);
-    }
-  } else {
-    MOZ_ASSERT(byteSize == 2 || byteSize == 4);
-    emitUnalignedStore(&access, byteSize, ptr, valOrTmp);
-  }
-
-  asMasm().memoryBarrierAfter(access.sync());
-}
-
-void MacroAssemblerARM::emitUnalignedLoad(const wasm::MemoryAccessDesc* access,
-                                          bool isSigned, unsigned byteSize,
-                                          Register ptr, Register tmp,
-                                          Register dest, unsigned offset) {
-  // Preconditions.
-  MOZ_ASSERT(ptr != tmp);
-  MOZ_ASSERT(ptr != dest);
-  MOZ_ASSERT(tmp != dest);
-  MOZ_ASSERT(byteSize == 2 || byteSize == 4);
-  MOZ_ASSERT(offset == 0 || offset == 4);
-
-  // The trap metadata is only valid for the first instruction, so we must
-  // make the first instruction fault if any of them is going to fault.  Hence
-  // byte loads must be issued from high addresses toward low addresses (or we
-  // must emit metadata for each load).
-  //
-  // So for a four-byte load from address x we will emit an eight-instruction
-  // sequence:
-  //
-  //   ldrsb [x+3], tmp           // note signed load *if appropriate*
-  //   lsl dest, tmp lsl 24       // move high byte + sign bits into place;
-  //                              // clear low bits
-  //   ldrb [x+2], tmp            // note unsigned load
-  //   or dest, dest, tmp lsl 16  // add another byte
-  //   ldrb [x+1], tmp            // ...
-  //   or dest, dest, tmp lsl 8
-  //   ldrb [x], tmp
-  //   or dest, dest, tmp
-
-  ScratchRegisterScope scratch(asMasm());
-
-  int i = byteSize - 1;
-
-  BufferOffset load = ma_dataTransferN(IsLoad, 8, isSigned, ptr,
-                                       Imm32(offset + i), tmp, scratch);
-  if (access) {
-    append(*access, load.getOffset());
-  }
-  ma_lsl(Imm32(8 * i), tmp, dest);
-  --i;
-
-  while (i >= 0) {
-    ma_dataTransferN(IsLoad, 8, /*signed=*/false, ptr, Imm32(offset + i), tmp,
-                     scratch);
-    as_orr(dest, dest, lsl(tmp, 8 * i));
-    --i;
-  }
-}
-
-void MacroAssemblerARM::emitUnalignedStore(const wasm::MemoryAccessDesc* access,
-                                           unsigned byteSize, Register ptr,
-                                           Register val, unsigned offset) {
-  // Preconditions.
-  MOZ_ASSERT(ptr != val);
-  MOZ_ASSERT(byteSize == 2 || byteSize == 4);
-  MOZ_ASSERT(offset == 0 || offset == 4);
-
-  // See comments above.  Here an additional motivation is that no side
-  // effects should be observed if any of the stores would fault, so we *must*
-  // go high-to-low, we can't emit metadata for individual stores in
-  // low-to-high order.
-  //
-  // For a four-byte store to address x we will emit a seven-instruction
-  // sequence:
-  //
-  //   lsr  scratch, val, 24
-  //   strb [x+3], scratch
-  //   lsr  scratch, val, 16
-  //   strb [x+2], scratch
-  //   lsr  scratch, val, 8
-  //   strb [x+1], scratch
-  //   strb [x], val
-
-  // `val` may be scratch in the case when we store doubles.
-  SecondScratchRegisterScope scratch(asMasm());
-
-  for (int i = byteSize - 1; i > 0; i--) {
-    ma_lsr(Imm32(i * 8), val, scratch);
-    // Use as_dtr directly to avoid needing another scratch register; we can
-    // do this because `offset` is known to be small.
-    BufferOffset store = as_dtr(IsStore, 8, Offset, scratch,
-                                DTRAddr(ptr, DtrOffImm(offset + i)));
-    if (i == (int)byteSize - 1 && access) {
-      append(*access, store.getOffset());
-    }
-  }
-  as_dtr(IsStore, 8, Offset, val, DTRAddr(ptr, DtrOffImm(offset)));
-}
diff --git a/js/src/jit/arm/MacroAssembler-arm.h b/js/src/jit/arm/MacroAssembler-arm.h
--- a/js/src/jit/arm/MacroAssembler-arm.h
+++ b/js/src/jit/arm/MacroAssembler-arm.h
@@ -555,54 +555,17 @@ class MacroAssemblerARM : public Assembl
                     Register ptr, Register ptrScratch, AnyRegister outAny,
                     Register64 out64);
 
   // `valAny` is valid if and only if `val64` == Register64::Invalid().
   void wasmStoreImpl(const wasm::MemoryAccessDesc& access, AnyRegister valAny,
                      Register64 val64, Register memoryBase, Register ptr,
                      Register ptrScratch);
 
- protected:
-  // `outAny` is valid if and only if `out64` == Register64::Invalid().
-  void wasmUnalignedLoadImpl(const wasm::MemoryAccessDesc& access,
-                             Register memoryBase, Register ptr,
-                             Register ptrScratch, AnyRegister outAny,
-                             Register64 out64, Register tmp1, Register tmp2,
-                             Register tmp3);
-
-  // The value to be stored is in `floatValue` (if not invalid), `val64` (if not
-  // invalid), or in `valOrTmp` (if `floatValue` and `val64` are both invalid).
-  // Note `valOrTmp` must always be valid.
-  void wasmUnalignedStoreImpl(const wasm::MemoryAccessDesc& access,
-                              FloatRegister floatValue, Register64 val64,
-                              Register memoryBase, Register ptr,
-                              Register ptrScratch, Register valOrTmp);
-
  private:
-  // Loads `byteSize` bytes, byte by byte, by reading from ptr[offset],
-  // applying the indicated signedness (defined by isSigned).
-  // - all three registers must be different.
-  // - tmp and dest will get clobbered, ptr will remain intact.
-  // - byteSize can be up to 4 bytes and no more (GPR are 32 bits on ARM).
-  // - offset can be 0 or 4
-  // If `access` is not null then emit the appropriate access metadata.
-  void emitUnalignedLoad(const wasm::MemoryAccessDesc* access, bool isSigned,
-                         unsigned byteSize, Register ptr, Register tmp,
-                         Register dest, unsigned offset = 0);
-
-  // Ditto, for a store. Note stores don't care about signedness.
-  // - the two registers must be different.
-  // - val will get clobbered, ptr will remain intact.
-  // - byteSize can be up to 4 bytes and no more (GPR are 32 bits on ARM).
-  // - offset can be 0 or 4
-  // If `access` is not null then emit the appropriate access metadata.
-  void emitUnalignedStore(const wasm::MemoryAccessDesc* access,
-                          unsigned byteSize, Register ptr, Register val,
-                          unsigned offset = 0);
-
   // Implementation for transferMultipleByRuns so we can use different
   // iterators for forward/backward traversals. The sign argument should be 1
   // if we traverse forwards, -1 if we traverse backwards.
   template <typename RegisterIterator>
   int32_t transferMultipleByRunsImpl(FloatRegisterSet set, LoadStore ls,
                                      Register rm, DTMMode mode, int32_t sign) {
     MOZ_ASSERT(sign == 1 || sign == -1);
 
diff --git a/js/src/wasm/WasmBaselineCompile.cpp b/js/src/wasm/WasmBaselineCompile.cpp
--- a/js/src/wasm/WasmBaselineCompile.cpp
+++ b/js/src/wasm/WasmBaselineCompile.cpp
@@ -7019,18 +7019,17 @@ class BaseCompiler final : public BaseCo
     if (dest.tag == AnyReg::I64) {
       MOZ_ASSERT(dest.i64() == specific_.abiReturnRegI64);
       masm.wasmLoadI64(*access, srcAddr, dest.i64());
     } else {
       // For 8 bit loads, this will generate movsbl or movzbl, so
       // there's no constraint on what the output register may be.
       masm.wasmLoad(*access, srcAddr, dest.any());
     }
-#elif defined(JS_CODEGEN_ARM) || defined(JS_CODEGEN_MIPS32) || \
-    defined(JS_CODEGEN_MIPS64)
+#elif defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)
     if (IsUnaligned(*access)) {
       switch (dest.tag) {
         case AnyReg::I64:
           masm.wasmUnalignedLoadI64(*access, HeapReg, ptr, ptr, dest.i64(),
                                     temp1);
           break;
         case AnyReg::F32:
           masm.wasmUnalignedLoadFP(*access, HeapReg, ptr, ptr, dest.f32(),
@@ -7048,35 +7047,37 @@ class BaseCompiler final : public BaseCo
       }
     } else {
       if (dest.tag == AnyReg::I64) {
         masm.wasmLoadI64(*access, HeapReg, ptr, ptr, dest.i64());
       } else {
         masm.wasmLoad(*access, HeapReg, ptr, ptr, dest.any());
       }
     }
+#elif defined(JS_CODEGEN_ARM)
+    if (dest.tag == AnyReg::I64) {
+      masm.wasmLoadI64(*access, HeapReg, ptr, ptr, dest.i64());
+    } else {
+      masm.wasmLoad(*access, HeapReg, ptr, ptr, dest.any());
+    }
 #elif defined(JS_CODEGEN_ARM64)
     if (dest.tag == AnyReg::I64) {
       masm.wasmLoadI64(*access, HeapReg, ptr, dest.i64());
     } else {
       masm.wasmLoad(*access, HeapReg, ptr, dest.any());
     }
 #else
     MOZ_CRASH("BaseCompiler platform hook: load");
 #endif
 
     return true;
   }
 
   RegI32 needStoreTemp(const MemoryAccessDesc& access, ValType srcType) {
-#if defined(JS_CODEGEN_ARM)
-    if (IsUnaligned(access) && srcType != ValType::I32) {
-      return needI32();
-    }
-#elif defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)
+#if defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)
     return needI32();
 #endif
     return RegI32::Invalid();
   }
 
   // ptr and src must not be the same register.
   // This may destroy ptr and src.
   [[nodiscard]] bool store(MemoryAccessDesc* access, AccessCheck* check,
@@ -7111,46 +7112,23 @@ class BaseCompiler final : public BaseCo
         value = AnyRegister(scratch);
       } else {
         value = src.any();
       }
 
       masm.wasmStore(*access, value, dstAddr);
     }
 #elif defined(JS_CODEGEN_ARM)
-    if (IsUnaligned(*access)) {
-      switch (src.tag) {
-        case AnyReg::I64:
-          masm.wasmUnalignedStoreI64(*access, src.i64(), HeapReg, ptr, ptr,
-                                     temp);
-          break;
-        case AnyReg::F32:
-          masm.wasmUnalignedStoreFP(*access, src.f32(), HeapReg, ptr, ptr,
-                                    temp);
-          break;
-        case AnyReg::F64:
-          masm.wasmUnalignedStoreFP(*access, src.f64(), HeapReg, ptr, ptr,
-                                    temp);
-          break;
-        case AnyReg::I32:
-          MOZ_ASSERT(temp.isInvalid());
-          masm.wasmUnalignedStore(*access, src.i32(), HeapReg, ptr, ptr, temp);
-          break;
-        default:
-          MOZ_CRASH("Unexpected type");
-      }
+    MOZ_ASSERT(temp.isInvalid());
+    if (access->type() == Scalar::Int64) {
+      masm.wasmStoreI64(*access, src.i64(), HeapReg, ptr, ptr);
+    } else if (src.tag == AnyReg::I64) {
+      masm.wasmStore(*access, AnyRegister(src.i64().low), HeapReg, ptr, ptr);
     } else {
-      MOZ_ASSERT(temp.isInvalid());
-      if (access->type() == Scalar::Int64) {
-        masm.wasmStoreI64(*access, src.i64(), HeapReg, ptr, ptr);
-      } else if (src.tag == AnyReg::I64) {
-        masm.wasmStore(*access, AnyRegister(src.i64().low), HeapReg, ptr, ptr);
-      } else {
-        masm.wasmStore(*access, src.any(), HeapReg, ptr, ptr);
-      }
+      masm.wasmStore(*access, src.any(), HeapReg, ptr, ptr);
     }
 #elif defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)
     if (IsUnaligned(*access)) {
       switch (src.tag) {
         case AnyReg::I64:
           masm.wasmUnalignedStoreI64(*access, src.i64(), HeapReg, ptr, ptr,
                                      temp);
           break;


